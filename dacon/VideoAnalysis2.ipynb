{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import mediapipe as mp\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams.update({\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'axes.spines.left': False,\n",
    "    'axes.spines.bottom': False,\n",
    "    'xtick.labelbottom': False,\n",
    "    'xtick.bottom': False,\n",
    "    'ytick.labelleft': False,\n",
    "    'ytick.left': False,\n",
    "    'xtick.labeltop': False,\n",
    "    'xtick.top': False,\n",
    "    'ytick.labelright': False,\n",
    "    'ytick.right': False\n",
    "})\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "\n",
    "def display_one_image(image, title, subplot, titlesize=16):\n",
    "    \"\"\"Displays one image along with the predicted category name and score.\"\"\"\n",
    "    plt.subplot(*subplot)\n",
    "    plt.imshow(image)\n",
    "    if len(title) > 0:\n",
    "        plt.title(title, fontsize=int(titlesize), color='black', fontdict={'verticalalignment':'center'}, pad=int(titlesize/1.5))\n",
    "    return (subplot[0], subplot[1], subplot[2]+1)\n",
    "\n",
    "\n",
    "def display_batch_of_images_with_gestures_and_hand_landmarks(images, results):\n",
    "    \"\"\"Displays a batch of images with the gesture category and its score along with the hand landmarks.\"\"\"\n",
    "    # Images and labels.\n",
    "    images = [image.numpy_view() for image in images]\n",
    "    gestures = [top_gesture for (top_gesture, _) in results]\n",
    "    multi_hand_landmarks_list = [multi_hand_landmarks for (_, multi_hand_landmarks) in results]\n",
    "\n",
    "    # Auto-squaring: this will drop data that does not fit into square or square-ish rectangle.\n",
    "    rows = int(math.sqrt(len(images)))\n",
    "    cols = len(images) // rows\n",
    "\n",
    "    # Size and spacing.\n",
    "    FIGSIZE = 13.0\n",
    "    SPACING = 0.1\n",
    "    subplot=(rows,cols, 1)\n",
    "    if rows < cols:\n",
    "        plt.figure(figsize=(FIGSIZE,FIGSIZE/cols*rows))\n",
    "    else:\n",
    "        plt.figure(figsize=(FIGSIZE/rows*cols,FIGSIZE))\n",
    "\n",
    "    # Display gestures and hand landmarks.\n",
    "    for i, (image, gestures) in enumerate(zip(images[:rows*cols], gestures[:rows*cols])):\n",
    "        title = f\"{gestures.category_name} ({gestures.score:.2f})\"\n",
    "        dynamic_titlesize = FIGSIZE*SPACING/max(rows,cols) * 40 + 3\n",
    "        annotated_image = image.copy()\n",
    "\n",
    "        for hand_landmarks in multi_hand_landmarks_list[i]:\n",
    "          hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "          hand_landmarks_proto.landmark.extend([            landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks          ])\n",
    "\n",
    "          mp_drawing.draw_landmarks(\n",
    "            annotated_image,\n",
    "            hand_landmarks_proto,\n",
    "            mp_hands.HAND_CONNECTIONS,\n",
    "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
    "            mp_drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        subplot = display_one_image(annotated_image, title, subplot, titlesize=dynamic_titlesize)\n",
    "\n",
    "    # Layout.\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=SPACING, hspace=SPACING)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [7.50, 3.50]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "\n",
    "def check_again(video_file_path):\n",
    "  \n",
    "  mp_drawing = mp.solutions.drawing_utils\n",
    "  mp_drawing_styles = mp.solutions.drawing_styles\n",
    "  mp_hands = mp.solutions.hands\n",
    "  cap = cv2.VideoCapture(video_file_path)\n",
    "  arr_res_finger_first = [] #4\n",
    "  arr_res_finger_fourth = [] #5,6\n",
    "  \n",
    "  with mp_hands.Hands(\n",
    "    model_complexity=0,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as hands:\n",
    "    while cap.isOpened():\n",
    "      \n",
    "      success, image = cap.read()\n",
    "      if not success:\n",
    "        print(\"Ignoring empty camera frame.\") \n",
    "        break;\n",
    " \n",
    "      image.flags.writeable = False\n",
    "      image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "      results = hands.process(image)\n",
    "\n",
    "      # Draw the hand annotations on the image.\n",
    "      image.flags.writeable = True\n",
    "      image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "      enum_finger_thumb = 4\n",
    "      enum_finger_second= 6\n",
    "      \n",
    "\n",
    "      arr_res_finger_first.append(results.multi_hand_landmarks[0].landmark[enum_finger_thumb].z)\n",
    "      arr_res_finger_fourth.append(results.multi_hand_landmarks[0].landmark[enum_finger_second].z)\n",
    "\n",
    "      cv2.imshow('MediaPipe Hands', cv2.flip(image, 1))\n",
    "      if cv2.waitKey(5) & 0xFF == 27:\n",
    "        break\n",
    "  #print(arr_res_finger_first)\n",
    "  #print(arr_res_finger_fourth)\n",
    "  i = 0\n",
    "  thumb_down_score = 0\n",
    "  thumb_up_score =  0\n",
    "  for a in arr_res_finger_first:\n",
    "    i=i+1\n",
    "    if arr_res_finger_fourth[i] - arr_res_finger_first[i] > 5:\n",
    "      thumb_down_score += 1\n",
    "    elif arr_res_finger_fourth[i] - arr_res_finger_first[i] < -1:\n",
    "      thumb_up_score += 1\n",
    "  \n",
    "  plt.title(\"Line graph\")\n",
    "  plt.plot(arr_res_finger_first, color=\"blue\")\n",
    "  plt.plot(arr_res_finger_fourth, color=\"red\")\n",
    "  \n",
    "  plt.show()\n",
    "  \n",
    "  res = 0\n",
    "  \n",
    "  if thumb_down_score > thumb_up_score : res = 1\n",
    "  elif thumb_down_score < thumb_up_score : res = 0\n",
    "  return res\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def do_process(video_file_path):\n",
    "  print(video_file_path)\n",
    "\n",
    "  import math\n",
    "  import cv2\n",
    "\n",
    "  import mediapipe as mp\n",
    "  from mediapipe.tasks import python\n",
    "  from mediapipe.tasks.python import vision\n",
    "  from mediapipe.python import packet_creator\n",
    "  from mediapipe.python import packet_getter\n",
    "\n",
    "  from mediapipe.python._framework_bindings import calculator_graph\n",
    "  from mediapipe.python._framework_bindings import image\n",
    "  from mediapipe.python._framework_bindings import image_frame\n",
    "  from mediapipe.python._framework_bindings import packet\n",
    "\n",
    "  CalculatorGraph = calculator_graph.CalculatorGraph\n",
    "  Image = image.Image\n",
    "  ImageFormat = image_frame.ImageFormat\n",
    "  ImageFrame = image_frame.ImageFrame\n",
    "\n",
    "  # STEP 2: Create an GestureRecognizer object.\n",
    "  VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "  base_options = python.BaseOptions(model_asset_path='/Users/s/dacon/gesture_recognizer.task')\n",
    "  options = vision.GestureRecognizerOptions(base_options=base_options, running_mode=VisionRunningMode.VIDEO )\n",
    "  recognizer = vision.GestureRecognizer.create_from_options(options)\n",
    "\n",
    "  results = []\n",
    "  \n",
    "  cap = cv2.VideoCapture(video_file_path)\n",
    "  fps = cv2.CAP_PROP_FPS\n",
    "  calc_timestamps = [0.0]\n",
    "\n",
    "  res_landmark = []\n",
    "  res_not_found_count = 0\n",
    "  \n",
    "  ret = True\n",
    "  while ret:\n",
    "    ret, img = cap.read() # read one frame from the 'capture' object; img is (H, W, C)\n",
    "    if ret:      \n",
    "      ts = cap.get(cv2.CAP_PROP_POS_MSEC)      \n",
    "      cts = calc_timestamps[-1] + 1000/fps\n",
    "      #print(abs(ts-cts))\n",
    "      recognition_result =recognizer.recognize_for_video( mp.Image(image_format=ImageFormat.SRGB, data=img), int(ts))\n",
    "      try : \n",
    "        top_gesture = recognition_result.gestures[0][0]        \n",
    "        hand_landmarks = top_gesture.category_name\n",
    "        #print(hand_landmarks)\n",
    "        results.append(top_gesture)\n",
    "        res_landmark.append(hand_landmarks)\n",
    "      except IndexError: \n",
    "        res_landmark.append('-')\n",
    "        res_not_found_count += 1\n",
    "        continue\n",
    "      \n",
    "  cap.release()\n",
    "  cv2.destroyAllWindows\n",
    "  res = -1\n",
    "  \n",
    "  if res_landmark.count('-') == len(res_landmark) :\n",
    "    print(\"None!\")\n",
    "    res = check_again(video_file_path)\n",
    "  else:    \n",
    "    print(res_landmark)    \n",
    "    if res_landmark.count('Open_Palm') > 10 : \n",
    "      res = 4 # Open_palm\n",
    "  return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./open/train/TRAIN_000.mp4\n",
      "None!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before InitGoogleLogging() is written to STDERR\n",
      "W20230205 23:52:37.271409 370468352 gesture_recognizer_graph.cc:121] Hand Gesture Recognizer contains CPU only ops. Sets HandGestureRecognizerGraph acceleartion to Xnnpack.\n",
      "I20230205 23:52:37.272728 370468352 hand_gesture_recognizer_graph.cc:249] Custom gesture classifier is not defined.\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "OpenCV: Couldn't read video stream from file \"./open/train/TRAIN_000.mp4\"\n",
      "OpenCV: Couldn't read video stream from file \"./open/train/TRAIN_000.mp4\"\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main2():\n",
    "    file_index = ''\n",
    "    for i in range(0, 1): \n",
    "        if i < 10 : file_index = '00'+str(i)\n",
    "        elif i> 10 and i<100: file_index= '0'+str(i)\n",
    "        else: file_index = str(i)\n",
    "        video_file_path = './open/train/TRAIN_'+file_index + '.mp4'\n",
    "        \n",
    "        do_process(video_file_path)\n",
    "        \n",
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "def main():\n",
    "  res_true = 0\n",
    "  res_false= 0\n",
    "  df = pd.read_csv('/Users/s/dacon/open/train.csv')\n",
    "  \n",
    "\n",
    "  for row in df.iterrows():\n",
    "    print(row[1].path)\n",
    "            \n",
    "    if do_process(row[1].path) == row[1].label:\n",
    "      res_true +=1\n",
    "    else:\n",
    "      res_false += 1\n",
    "      \n",
    "if __name__ == \"__main__\":\n",
    "  main2()\n",
    " \n",
    "    #print(res_not_found_count)\n",
    "    #print(res_landmark.count('Open_Palm'))\n",
    "    #print(results)\n",
    "    \n",
    "#  display_batch_of_images_with_gestures_and_hand_landmarks(images, results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4806d66191694be8207ed94abd53c19017dbadabf90114378400412e75f1ddd5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
