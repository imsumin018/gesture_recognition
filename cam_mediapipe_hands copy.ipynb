{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/imsumin018/gesture_recognition/blob/main/mediapipe_hands.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2PlazN5Q94l"
      },
      "source": [
        "Usage example of MediaPipe Hands Solution API in Python (see also http://solutions.mediapipe.dev/hands)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "AuQfLvpuJkb0",
        "outputId": "d02ce647-67d3-4b3a-db78-3b688c8b36e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mediapipe\n",
            "  Using cached mediapipe-0.9.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.0 MB)\n",
            "Collecting protobuf<4,>=3.11\n",
            "  Using cached protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "Collecting absl-py\n",
            "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
            "Collecting opencv-contrib-python\n",
            "  Using cached opencv_contrib_python-4.7.0.68-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (67.9 MB)\n",
            "Collecting flatbuffers>=2.0\n",
            "  Using cached flatbuffers-23.1.21-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /home/bketc/.virtualenvs/p2023Jan/lib/python3.8/site-packages (from mediapipe) (22.2.0)\n",
            "Collecting matplotlib\n",
            "  Using cached matplotlib-3.6.3-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.4 MB)\n",
            "Collecting numpy\n",
            "  Using cached numpy-1.24.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "Collecting cycler>=0.10\n",
            "  Using cached cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
            "Collecting pyparsing>=2.2.1\n",
            "  Using cached pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
            "Collecting pillow>=6.2.0\n",
            "  Downloading Pillow-9.4.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /home/bketc/.virtualenvs/p2023Jan/lib/python3.8/site-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Collecting kiwisolver>=1.0.1\n",
            "  Using cached kiwisolver-1.4.4-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
            "Collecting fonttools>=4.22.0\n",
            "  Using cached fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
            "Collecting contourpy>=1.0.1\n",
            "  Using cached contourpy-1.0.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/bketc/.virtualenvs/p2023Jan/lib/python3.8/site-packages (from matplotlib->mediapipe) (23.0)\n",
            "Requirement already satisfied: six>=1.5 in /home/bketc/.virtualenvs/p2023Jan/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
            "Installing collected packages: flatbuffers, pyparsing, protobuf, pillow, numpy, kiwisolver, fonttools, cycler, absl-py, opencv-contrib-python, contourpy, matplotlib, mediapipe\n",
            "Successfully installed absl-py-1.4.0 contourpy-1.0.7 cycler-0.11.0 flatbuffers-23.1.21 fonttools-4.38.0 kiwisolver-1.4.4 matplotlib-3.6.3 mediapipe-0.9.0.1 numpy-1.24.1 opencv-contrib-python-4.7.0.68 pillow-9.4.0 protobuf-3.20.3 pyparsing-3.0.9\n"
          ]
        }
      ],
      "source": [
        "!pip install mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "mp_hands = mp.solutions.hands\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "id": "nW2TjFyhLvVH",
        "outputId": "85eb8b35-4072-49cb-ea48-c1ba3cc0069e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m image\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mwriteable \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     17\u001b[0m image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(image, cv2\u001b[39m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m---> 18\u001b[0m results \u001b[39m=\u001b[39m hands\u001b[39m.\u001b[39;49mprocess(image)\n\u001b[1;32m     20\u001b[0m \u001b[39m# Draw the hand annotations on the image.\u001b[39;00m\n\u001b[1;32m     21\u001b[0m image\u001b[39m.\u001b[39mflags\u001b[39m.\u001b[39mwriteable \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/.virtualenvs/p2023Jan/lib/python3.8/site-packages/mediapipe/python/solutions/hands.py:153\u001b[0m, in \u001b[0;36mHands.process\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess\u001b[39m(\u001b[39mself\u001b[39m, image: np\u001b[39m.\u001b[39mndarray) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m NamedTuple:\n\u001b[1;32m    133\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \n\u001b[1;32m    135\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39m         right hand) of the detected hand.\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mprocess(input_data\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mimage\u001b[39;49m\u001b[39m'\u001b[39;49m: image})\n",
            "File \u001b[0;32m~/.virtualenvs/p2023Jan/lib/python3.8/site-packages/mediapipe/python/solution_base.py:365\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m    359\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    360\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39madd_packet_to_input_stream(\n\u001b[1;32m    361\u001b[0m         stream\u001b[39m=\u001b[39mstream_name,\n\u001b[1;32m    362\u001b[0m         packet\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_packet(input_stream_type,\n\u001b[1;32m    363\u001b[0m                                  data)\u001b[39m.\u001b[39mat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_simulated_timestamp))\n\u001b[0;32m--> 365\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph\u001b[39m.\u001b[39;49mwait_until_idle()\n\u001b[1;32m    366\u001b[0m \u001b[39m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[39m# output stream names.\u001b[39;00m\n\u001b[1;32m    368\u001b[0m solution_outputs \u001b[39m=\u001b[39m collections\u001b[39m.\u001b[39mnamedtuple(\n\u001b[1;32m    369\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mSolutionOutputs\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_stream_type_info\u001b[39m.\u001b[39mkeys())\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "\n",
        "# For webcam input:\n",
        "cap = cv2.VideoCapture(0)\n",
        "with mp_hands.Hands(\n",
        "    model_complexity=0,\n",
        "    min_detection_confidence=0.5,\n",
        "    min_tracking_confidence=0.5) as hands:\n",
        "  while cap.isOpened():\n",
        "    success, image = cap.read()\n",
        "    if not success:\n",
        "      print(\"Ignoring empty camera frame.\")\n",
        "      # If loading a video, use 'break' instead of 'continue'.\n",
        "      continue\n",
        "\n",
        "    # To improve performance, optionally mark the image as not writeable to\n",
        "    # pass by reference.\n",
        "    image.flags.writeable = False\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    results = hands.process(image)\n",
        "\n",
        "    # Draw the hand annotations on the image.\n",
        "    image.flags.writeable = True\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "    if results.multi_hand_landmarks:\n",
        "      for hand_landmarks in results.multi_hand_landmarks:\n",
        "        mp_drawing.draw_landmarks(\n",
        "            image,\n",
        "            hand_landmarks,\n",
        "            mp_hands.HAND_CONNECTIONS,\n",
        "            mp_drawing_styles.get_default_hand_landmarks_style(),\n",
        "            mp_drawing_styles.get_default_hand_connections_style())\n",
        "    # Flip the image horizontally for a selfie-view display.\n",
        "    cv2.imshow('MediaPipe Hands', cv2.flip(image, 1))\n",
        "    if cv2.waitKey(5) & 0xFF == 27:\n",
        "      break\n",
        "cap.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Fso4oNqXIkp"
      },
      "source": [
        "All MediaPipe Solutions Python API examples are under mp.solutions.\n",
        "\n",
        "For the MediaPipe Hands solution, we can access this module as `mp_hands = mp.solutions.hands`.\n",
        "\n",
        "You may change the parameters, such as `static_image_mode`, `max_num_hands`, and `min_detection_confidence`, during the initialization. Run `help(mp_hands.Hands)` to get more informations about the parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "BboTB-FAMfPo",
        "outputId": "61e11c6a-1c29-40cc-bb4b-c73efdd8d14f"
      },
      "outputs": [],
      "source": [
        "import mediapipe as mp\n",
        "mp_hands = mp.solutions.hands\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "help(mp_hands.Hands)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 757
        },
        "id": "BAivyQ_xOtFp",
        "outputId": "fc29b62f-1a76-4ea7-e148-9083050e26eb"
      },
      "outputs": [],
      "source": [
        "# Run MediaPipe Hands.\n",
        "with mp_hands.Hands(\n",
        "    static_image_mode=True,\n",
        "    max_num_hands=2,\n",
        "    min_detection_confidence=0.7) as hands:\n",
        "  for name, image in images.items():\n",
        "    # Convert the BGR image to RGB, flip the image around y-axis for correct \n",
        "    # handedness output and process it with MediaPipe Hands.\n",
        "    results = hands.process(cv2.flip(cv2.cvtColor(image, cv2.COLOR_BGR2RGB), 1))\n",
        "\n",
        "    # Print handedness (left v.s. right hand).\n",
        "    print(f'Handedness of {name}:')\n",
        "    print(results.multi_handedness)\n",
        "\n",
        "    if not results.multi_hand_landmarks:\n",
        "      continue\n",
        "    # Draw hand landmarks of each hand.\n",
        "    print(f'Hand landmarks of {name}:')\n",
        "    image_hight, image_width, _ = image.shape\n",
        "    annotated_image = cv2.flip(image.copy(), 1)\n",
        "    for hand_landmarks in results.multi_hand_landmarks:\n",
        "      # Print index finger tip coordinates.\n",
        "      print(\n",
        "          f'Index finger tip coordinate: (',\n",
        "          f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].x * image_width}, '\n",
        "          f'{hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP].y * image_hight})'\n",
        "      )\n",
        "      mp_drawing.draw_landmarks(\n",
        "          annotated_image,\n",
        "          hand_landmarks,\n",
        "          mp_hands.HAND_CONNECTIONS,\n",
        "          mp_drawing_styles.get_default_hand_landmarks_style(),\n",
        "          mp_drawing_styles.get_default_hand_connections_style())\n",
        "    resize_and_show(cv2.flip(annotated_image, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1150
        },
        "id": "LAchzK23Uabf",
        "outputId": "4a58589c-dc4c-45a4-d5fd-aaebe317baee"
      },
      "outputs": [],
      "source": [
        "# Run MediaPipe Hands and plot 3d hands world landmarks.\n",
        "with mp_hands.Hands(\n",
        "    static_image_mode=True,\n",
        "    max_num_hands=2,\n",
        "    min_detection_confidence=0.7) as hands:\n",
        "  for name, image in images.items():\n",
        "    # Convert the BGR image to RGB and process it with MediaPipe Hands.\n",
        "    results = hands.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "    # Draw hand world landmarks.\n",
        "    print(f'Hand world landmarks of {name}:')\n",
        "    if not results.multi_hand_world_landmarks:\n",
        "      continue\n",
        "    for hand_world_landmarks in results.multi_hand_world_landmarks:\n",
        "      mp_drawing.plot_landmarks(\n",
        "        hand_world_landmarks, mp_hands.HAND_CONNECTIONS, azimuth=5)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "p2023Jan",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "3e2aa766ae7d0e61af1bfc04b894080aa4be5333e5525c8350788f1140453135"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
